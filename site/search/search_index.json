{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Gaby Agrocostea's Data Science Portfolio","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#about-me","title":"About me","text":"<p>I'm a data scientist, instructor and mentor. During my free time, I teach data science, python, machine learning and SQL to beginner and advanced students as a consultant. </p> <p>I enjoy writing blogs about latest developments in data science and machine learning, sharing insights from my current and past projects, and discussing my experiences in teaching and mentoring. Sharing knowledge with others is a passion of mine! You can find my blog posts on medium.com/gabya06 </p>"},{"location":"#current-projects","title":"Current Projects","text":"<p>The project I am currently working on</p>"},{"location":"#previous-projects","title":"Previous Projects","text":"<p>Documentation and code for my previous work</p> <ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"ensemble/readme/","title":"Ensemble regression models for price forecasting","text":"<p>Ensemble methods are very popular and effective methods in machine learning, and in this post I'm going to illustrate how to forecast prices using ensemble learning.</p> <p>While time series and regression models work well on their own, when combining these using ensemble methods we can get a better performing model and more accurate predictions for tomorrow's price values given historical data.</p> <p>In this post, I will cover:</p> <li> Ensemble methods basics and fundamentals</li> <li> Python code for ensembles using scikit-learn machine learning library</li> <li> Ensemble Regression use case for price forecasting</li>"},{"location":"ensemble/readme/#what-are-ensemble-models","title":"What are ensemble models?","text":"<p>Ensemble models combine predictions from other simpler models to produce better predictions. There are two types of ensemble methods:</p> <p>1) parallel ensemble methods which combine independent learners to reduce variance 2) sequential ensemble methods which combine models in order to reduce bias and variance</p> <p>Parallel ensemble methods generate and train their base learners in parallel and these learners are independent of each other. Training parallel base estimators is also known as Bagging (Bootstrap Aggregation) and here we aggregate/average the individual base estimator predictions into one final prediction.</p> <p>It is referred to as Bootstrap Aggregation since bootstrap sampling is used to sample the data subsets needed to train the base learners; simply put, bootstrapping sampling refers to sampling with replacement while assigning measures of accuracy to sample estimates.</p> <p>The combined estimator uses averaging for regression problems and majority voting for classification problems; and this is usually better than the single base estimators because variance is reduced by averaging. Random Forest is an example of an effective bagging algorithm which aggregates over multiple decision trees, each trained using different bootstrapped samples as well as split on different features.</p> <p></p> <p>As the names suggests, sequential ensemble methods generate and train their base learners sequentially (one after the other), and these learners are dependent of each another. As such, the overall performance can be improved by weighing previously mislabeled/incorrect examples with higher weight; the goal is to reduce the bias of the combined estimator.</p> <p>This method is also known as Boosting and AdaBoost is an example of a popular and effective Boosting algorithm. The base learners are usually weak learners such as small decision trees trained on a weighted version of the data based on the previous learners' mistakes. Weights are increased based on correct and incorrect predictions, and more weight is given to more accurate learners. The final predictions are combined by a weighted sum or weighted majority vote.</p> <p>Another example of a boosting algorithm is Gradient Boosting, which is the model used in this price forecasting use case. Since prices are predicted, the values (also known as labels) are numeric. As such, regression models are used as the base learners and then combined using Gradient Boosting Regressor.</p> <p>Below is a basic workflow of how models could feed base predictions to an ensemble learner which then would make final price predictions. In this illustration have chosen my base learners to be linear regression, moving averages, simple exponential smoothing and LSTM, but there are other models that could be used.</p> <p></p>"},{"location":"ensemble/readme/#scikit-learn-api-for-ensemble-methods","title":"Scikit-Learn API for Ensemble Methods","text":"<p>The scikit-learn python machine learning library provides Gradient Boosting ensembles methods for both classification and regression problems.</p> <p>In the below example, after our import statements, we read the training data for our ensemble regressor - which is the output of our base model predictions - and then split the dataset into X and y.</p> <pre><code># import statements\nimport pandas as pd\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n# read csv file\ndf = pd.read_csv(\"price_data.csv\")\n</code></pre> <p></p> <p>In this case, the ensemble model is defined using 500 estimators and uses Absolute Error as its loss function. It is then fit to our data with one simple line of code. The next line of code makes predictions on our training data.</p> <p>Please note that this is meant as a very simple example and that usually we need include additional steps into our modeling such as splitting the data into training and testing, cross validation and analyze various error metrics for performance.</p> <pre><code># define model - let's start with 500 estimators\nensemble_regressor = GradientBoostingRegressor(random_state=23, loss='absolute_error',\nn_estimators=500)\n\n# fit the model\nensemble_regressor.fit(X, y)\n\n# make predictions\nensemble_preds = ensemble_regressor.predict(X)\n</code></pre> <p>There are multiple hyper parameters that can be tuned such as the loss function, learning rate, min samples split, max depth and others.</p> <p>The default loss function is squared error for regression, but we can chose from least absolute error, huber or quantile. The default criterion which measures the quality of a split is \"friedman_mse\" but we could have also picked \"squared_error\" instead.</p> <p>In scikit-learn we see these parameters:</p> <p></p>"},{"location":"ensemble/readme/#ensemble-regression-case-study","title":"Ensemble Regression Case Study","text":"<p>As a quick recap, we now have a better understanding of what ensemble models are: decision trees combined to produce better predictions using averages or weighted averages in the case of AdaBoost. Now, we can take a look at our use case for price prediction.</p> <p>For base models, I used several linear regression models as well as time series models. While linear regression models are easy to fit, fast in runtime and great for understanding general trends, they don't always capture granular changes. Time series models are good for repeating seasonal trends; however this can also be considered a disadvantage if the model holds on to trends longterm. Knowing this bit of information, it is useful to add a quick learner such as an LSTM model. Moreover, one can start to understand why it might be advantageous to combine the pros of these models somehow.</p> <p>Having cleaned and vectorized data, we split the data for each model into training and test data sets. In this example, linear regression models trained across different time periods (e.g: say we have two models, one trained on 3 months and one trained on 9 months), so we have to be careful about splitting our data accordingly for each model.</p> <p>Each model is trained and then makes predictions against the test data set. The ensemble model is consequently trained; the feature set consists of these base model predictions and the dependent variable y consists of the test data price values. Lastly the trained ensemble learner returns one target price for the interval forecast period of 30 days.</p> <p></p> <p>Price prediction is a common use case and we can imagine how implementing these models can be leveraged to allow different types of clients and Account Management teams to better auto-scale pricing for their products. This is one of many interesting use cases of ensemble regression for price forecasting.</p>"},{"location":"ensemble/readme/#additional-resources","title":"Additional Resources:","text":"<ul> <li>Scikit-learn Ensemble Doumentation</li> <li>Kaggle Ensemble Documentation</li> </ul>"},{"location":"genre_predictions/readme/","title":"Predicting Movie Genres using Machine Learning Models and Semantic Textual Similarity","text":"<p>Some people like scary movies, others like drama and some just love sports. Some can't wait for Christmas time to watch those feel good movies while others can't wait for Halloween horror movies. While we all have our favorite movie types, is there a way to know which genres are most popular? Do people actually watch Christmas movies when they first come on in late October? Netflix, Hulu and other providers already categorize shows for us, but how do they do this? In order to answer these questions and more, we need to be able to assign genres to tv shows and movies given a synopsis or description.</p> <p>In this post, I will discuss:</p> <li> Data pre-processing: Transforming show descriptions using TFID for machine learning model prediction</li> <li> Genre predictions using Naive Bayes and ensemble methods</li> <li> Semantic textual similarity/correlations: TensorFlow sentence encoder</li>"},{"location":"genre_predictions/readme/#data-collection-finding-show-descriptions-and-genres","title":"Data Collection - Finding Show Descriptions and Genres","text":"<p>The first task in predicting genres is finding show descriptions and their genres. The TMDB API is a free API built by the community and can be used to pull genre and overview information. It includes many other fields such as release date, keywords and language. While the data is not perfect data, it is still a good source of information.</p>"},{"location":"genre_predictions/readme/#data-preprocessing-and-cleaning-a-crucial-step","title":"Data Preprocessing and Cleaning - A Crucial Step","text":"<p>One very important preprocessing step before building any machine learning models or performing any analysis is data cleaning. The overview field describes what a show is about and can be input by anyone, so I had quite a bit of data cleaning to do, as would be expected with most NLP tasks.</p> <p>Data preprocessing involved the following steps (and more):</p> <li>Tokenization: Splitting show descriptions into words</li> <li>Removing stop words, punctuations and numbers</li> <li>Remove shows and descriptions that didn't provide any insight on show genre such as 'Synopsis missing' or ones that contained very few words</li> <p>The reason for this is that we don't want our model to use words or numbers that are not important in predicting a show's genre. For example, we remove words such as \"a\", \"the\", \"it\" (referred to as stop words) because they will not provide any important information in our predicting task.</p> <p>In the below example, I used the NLTK library to perform some of the preprocessing steps mentioned above. The NLTK library is very useful when you are working with natural language; it contains many algorithms which make preprocessing a lot easier.</p> <p></p> <p>A quick look at the data shows that each row in the tokens column contains a list of words: </p>"},{"location":"genre_predictions/readme/#tfidf-vectorization-term-frequency-inverse-document-frequency","title":"TFIDF Vectorization (Term Frequency-Inverse Document Frequency)","text":"<p>In general, machine learning models don't understand words as features, so we need to do something to convert words to features (a numeric representation). In order to use Naive Bayes or SVM classification models, each row with a synopsis needs to be converted to numeric representation - this is referred to as vectorization.</p> <p>The scikit-learn python machine learning library provides methods to convert words to numbers: we can use <code>CountVectorizer</code> to count the number of times a word appears or we can use <code>TfidfVectorizer</code> which converts words to TF-IDF representation.</p>"},{"location":"genre_predictions/readme/#a-little-about-tf-idf-term-frequency-inverse-document-frequency","title":"A little about TF-IDF-Term Frequency Inverse Document Frequency","text":"<p>TF-IDF can be thought of as the weighted score of a word in relation to the document. The idea is to calculate how important a word is to a document in a corpus. Mathematically the formula is:</p> <p>TF-IDF = TF(t,d) * IDF (t) = tf(t,d) * log(N/(df+1)</p> <p>Which can further be broken down into two parts:</p> <ul> <li>Tf(t,d) = term frequency: number of times term t appears in a doc d</li> <li>IDF(t,D) = Inverse document frequency: a measure of how much information a word provides based on the number of documents the term occurs in.</li> </ul> <p>Calculating this by hand seems daunting, but using Tf-Idf from the scikit-learn library is actually quite straightforward! We simply fit the vectorizer on our training corpus after importing and calling the vectorizer. In general, we refer to our text data in NLP problems as corpus/training corpus.</p> <p></p> <p>Next, we transform our corpus to tf-idf representation. This is just one line of code and this happens after we have done our data cleaning.</p> <p></p> <p>We can double check our new training dataset and see that it is now a sparse matrix with numeric datatype: </p> <p>If we wanted to see the features and vocabulary based on the corpus: </p> <p></p> <p>If we wanted to see all of the feature names: </p>"},{"location":"genre_predictions/readme/#and-finally-model-training","title":"And Finally ...  Model Training ...","text":"<p>After the data cleaning and vectorization, we can finally fit a Naive Bayes model in a few lines of code. We can make use of Pipeline to pass in the fit vectorizer along with our model:</p> <p></p> <p>To train our SVM model, we can use the above Pipeline approach above, or simply run the\u00a0.fit command:</p> <p></p>"},{"location":"genre_predictions/readme/#feature-engineering-and-model-improvements","title":"Feature Engineering and Model Improvements","text":"<p>To my surprise and disappointment, initially Naive Bayes and SVM classification models performed very poorly on both training and test data! In order to improve the model performance I had to go back and make several changes to the features and labels. Small changes such as combining \"action &amp; adventure\" with \"adventure\", \"SciFi\" with \"science fiction\" improved the model performance significantly. The idea was to reduce the number of labels to be predicted by combining some of our overlapping genres.</p> <p>As a final step, I trained a gradient boosting classification model on the combined output predictions of each model which further increased performance by 5% to about 80% accuracy.</p> <p>In the below, we see that sometimes the different models don't always predict the same genre, but when these predictions are combined as features in the ensemble model, it outperforms each one individually.</p> <p></p>"},{"location":"genre_predictions/readme/#semantic-textual-similarity-a-second-approach","title":"Semantic Textual Similarity - A second Approach","text":"<p>Using TensorFlow we can access The Universal Sentence Encoder and use it to obtain show genres for movies; and we can do this by calculating sentence similarities.</p> <p>This encoder model is different from other word level embeddings because it is trained on a number of natural language prediction tasks that require modeling the meaning of word sequences rather than just modeling individual words.</p> <p>In order to use the model, we have to first load it. You can load it once you have downloaded it to your local computer or as I have done below using the web url:</p> <p></p> <p>Once the model is loaded, embeddings can easily be produced for show overviews - it is almost like doing a simple lookup: all we need to do is provide the input as a list of sentences. In this case, this will be the cleaned up show descriptions that we would like our model to learn.</p> <p>The below code returns sentence embeddings for a list of sentences: </p>"},{"location":"genre_predictions/readme/#making-predictions-on-new-shows","title":"Making Predictions on New Shows","text":"<p>When we have a new show description that we would like to predict, we first need to obtain its embeddings using the same sentence encoder. Next, we take the inner product with the corpus embeddings of our training data. This will represent the semantic similarity between our new show description and our training data. </p> <p>Lastly, the genre of the training sentence with the highest similarity/correlation will be assigned as the genre for the new show. In the below example we can see that the show overview for Jane and the Dragon is most correlated with other another animation films, so we have assigned the new genre as animation: </p>"},{"location":"genre_predictions/readme/#resources","title":"Resources:","text":"<ul> <li>TMDB API</li> <li>TFID Vectorizer</li> <li>TensorFlow Semantic Similarity</li> </ul>"},{"location":"glassdoor_reviews/readme/","title":"Sentiment Analysis on glassdoor.com data science jobs","text":""},{"location":"glassdoor_reviews/readme/#project-overview","title":"Project Overview","text":"<p>The goal of this project was to scrape some reviews from glassdoor.com from data science job postings and to analyze company reviews and perform sentiment analysis to understand what users were writing. I also wanted to investigate how the number of stars given to a company related to the sentiment in the review.  I wrote an article on my page on medium.com and I also wanted to share the jupyter notebook.</p>"},{"location":"glassdoor_reviews/readme/#web-scrapping-glassdoor-data","title":"Web Scrapping Glassdoor Data","text":"<p>Since glassdoor.com does not have an available API, I decided to try out web scrapping on a few glassdoor.com machine learning job listings with company reviews.  I was only able to scrape a small amount of reviews before getting the 403 Forbidden Error. To scrape data I used Selenium and BeautifulSoup packages. Below are two of the main functions I used to scrape data using BeatufifulSoup:</p> <pre><code>def get_page(url, headers):\n    \"\"\"\n    Function to get webpage results into Beautifulsoup object\n    \"\"\"\n    try:\n        req = Request(url, headers=headers)\n        page = urlopen(req)\n        soup = BeautifulSoup(page, \"html.parser\")\n        return soup\n    except HTTPError as e:\n        print(f\"Error opening page {e}\")\n</code></pre> <p>To pull job listings I used the below:</p> <pre><code>def get_jobs():\n    \"\"\"\n    Function to get job listing table from glassdoor.com\n        It will load webpage first then loop through table\n        It will return data with columns:\n            [company, job_title, job_url, job_id]\n    \"\"\"\n\n    # load webpage using Beautifulsoup first\n    soup = get_page()\n    table = soup.find(\"table\")\n    table_rows = table.find_all(\"tr\")\n\n    data_dict = {}\n    data_list = []\n\n    # loop through table rows and extract information to dictionary\n    for elem in table_rows[1:]:\n        try:\n            data_dict[\"company\"] = elem.find(\"td\", {\"class\": \"company\"}).text\n        except AttributeError:\n            data_dict[\"company\"] = None\n        try:\n            data_dict[\"job_title\"] = elem.find(\"td\", {\"class\": \"job_title\"}).text\n        except AttributeError:\n            data_dict[\"job_title\"] = None\n        try:\n            data_dict[\"job_url\"] = elem.find(\"a\", {\"class\": \"jobLink\"})[\"href\"]\n        except AttributeError:\n            data_dict[\"job_url\"] = None\n\n        data_list.append(data_dict)\n        data_dict = {}\n    # put results in dataframe\n    data = pd.DataFrame(data_list)\n    # add column with job listing Id number\n    data = data.assign(\n        job_id=data.job_url.map(\n            lambda x: re.findall(pattern=\"jobListingId=[0-9]+\", string=x)[0]\n        ).map(lambda x: x.strip(\"jobListingId=\"))\n    )\n    # add column with url\n    listing_base_url = f\"https://www.glassdoor.com/job-listing/?jl=\"\n    data = data.assign(\n        url=data.job_id.map(lambda x: listing_base_url + str(x))\n    )\n\n    return data\n</code></pre>"},{"location":"glassdoor_reviews/readme/#text-preprocessing","title":"Text Preprocessing","text":"<p>In order to proceed with sentiment analysis, I first had to do some text preprocessing: remove punctuation, numbers and stop words. When working on NLP projects, I am usually a big fan of the NLTK library, but this time I wanted to try out TextBlob. Cleaning text data can be as simple as the below line of code:</p> <pre><code># assign new clean review column\ndf = df.assign(clean_review = df.reviews.map(lambda x: ' '.join(TextBlob(str(x)).words)))\n</code></pre> <p>In addition, I removed common words such \"a\", \"an\",\"the\",\"in\" which usually get ignored by typical tokenizers. These are called stopwords and NLTK includes a list of 40 stop words, but you can add more words and customize your own list of stopwords. </p> <pre><code># Remove stopwords &amp; lowercase\ndf.clean_review = df.clean_review.map(lambda x: \" \".join([i.lower() for i in x.split() if i not in stopwords.words('english')]))\n</code></pre>"},{"location":"glassdoor_reviews/readme/#data-exploration-visualization","title":"Data Exploration &amp; Visualization","text":"<p>Once the data is cleaned I can finally move on to the fun part, visualizations! I wanted to create a word cloud of the most frequent words in the reviews, so I created word counts:</p> <pre><code># collect words\nword_list = []\nword_list.extend(df.clean_review.str.split())\n# flatten list of words - exclude the word reviews\nword_list = [item for row in word_list for item in row if item !='reviews']\n# create counter - we want word frequencies\nword_counts = Counter(word_list)\n</code></pre> <p>And what words appear the most? \"good\", \"work\", \"people\", \"great\", \"benefits\", \"culture\", \"balance\", \"pay\", \"management\", \"life\" are all among the top 10 most common words in the dataset. This isnt surprising, I think it represents what most people are looking for in a good work environment. </p>"},{"location":"glassdoor_reviews/readme/#wordclouds","title":"WordClouds","text":"<p>Creating wordcloud visualizations are pretty straightforward once you have the word counts:</p> <pre><code># Wordcloud\nwordcloud = WordCloud(width = 300,\n                      height = 300,\n                      background_color='white',\n                      max_font_size=50, max_words=150)\n\nwordcloud = wordcloud.generate_from_frequencies(word_counts)\n\n# plot words\nplt.figure(figsize=(6,4),facecolor = 'white', edgecolor='blue')\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.title(\"Top Glassdoor.com words\")\nplt.savefig(\"review_wordcloud.png\")\nplt.show()\n</code></pre> <p></p>"},{"location":"glassdoor_reviews/readme/#sentiment-analysis-using-textblob-polarity-and-subjectivity","title":"Sentiment Analysis using TextBlob Polarity and Subjectivity","text":"<p>TextBlob provides the ability to look at sentiment analysis by breaking it down into polarity and subjectivity:</p> <ul> <li>Polarity is between -1 and 1: it identifies the most negative as -1 and positive as +1</li> <li>Subjectivity is between 0 and 1 and shows the amount of personal opinion: 0 is very objective while 1 is subjective</li> <li>TextBlob allows us to see the sentiment for each word using sentiment_assessment, so we can get a better understanding of how words are scored</li> </ul> <p>Finding the polarity and subjectivity in a string using TextBlob can be done in a few lines of code:</p> <pre><code>sample_string = df.clean_review.iloc[0]\n# polarity &amp; subjectivity:\nprint(f\"Sample Review:\\n{sample_string}\\nTextBlob polarity:{TextBlob(sample_string).sentiment.polarity}\"\n      f\" and subjectivity:{TextBlob(sample_string).subjectivity}\")\n\n# we can see the sentiment assessment of each word in our sample sentence\nTextBlob(sample_string).sentiment_assessments[2]\n</code></pre> <p>python output:</p> <p>Sample Review with 4.1 stars: good place retire since benefits good willing sacrifice salary/growth reviews great culture open learning environment reviews nice people work reviews very flexible good work life balance reviews good pay good hours reviews long hours culture staying late reviews difficult maintain good work/life balance reviews bad pay promotion hard get reviews TextBlob polarity:0.26 and subjectivity:0.63</p> <p>Note: the above sample review is cleaned: it has stopwords removed, no numbers and stripped whitespaces.</p> <p>Part of the sentiment assessment looks like this. It returns a tuple with (polarity, subjectivity, assessments). We see more negative words \"bad\", \"difficult\" and \"hard\" have negative polarity.</p> <p>[(['good'], 0.7, 0.60, None),  (['willing'], 0.25, 0.75, None),  (['great'], 0.8, 0.75, None),  (['open'], 0.0, 0.5, None),  (['nice'], 0.6, 1.0, None),  (['long'], -0.05, 0.4, None),  (['late'], -0.3, 0.6, None),  (['difficult'], -0.5, 1.0, None),  (['bad'], -0.699, 0.66, None),  (['hard'], -0.29, 0.54, None)]</p> <p>.... So far so good, it looks like TextBlob assigns a more positive sentiment score to the first review which makes sense and also aligns with the number of stars on glassdoor.com.</p>"},{"location":"glassdoor_reviews/readme/#but-can-we-trust-stars-given-in-reviews","title":"But can we trust stars given in reviews?","text":"<ul> <li>After plotting polarity and stars, we can see that polarity should be increasing as stars increase but this is not the case.</li> <li>Polarity is higher for 3.75 star rated reviews than for reviews with 4.0 stars. This is inconsistent with what we would expect.</li> <li>We also see the review for the \"worse\" rating of 3.1 does not correspond to a negative review</li> </ul> <pre><code># let's find worse review\nworse_stars = df.stars.min()\nworse_review = df[df.stars == df.stars.min()].clean_review.values[0]\nworse_company = df[df.stars == worse_stars].company.values[0]\nw_polarity = TextBlob(worse_review).polarity\nw_subjectivity = TextBlob(worse_review).subjectivity\n\n# here we can see that this review is not bad: Textblob identifies it as more positive than negative based on polarity closer to 1\nprint(f\"{worse_company} with {worse_stars} has the worse review:\\n{worse_review}\")\nprint(f\"\\nBut TextBlob indicates it has {w_polarity} polarity and {w_subjectivity}\")\n</code></pre> <p>python output:</p> <p>Epic Pharma LLC with 3.1 has the worse review:</p> <p>the people working company nice reviews 'no cons reported glassdoor community</p> <p>But TextBlob indicates it has 0.6 polarity and 1.0</p> <p>When plotting polarity and subjectivity vs. stars given we see that polarity should be increasing as stars increase but that is not always the case.  For example, polarity is higher for 3.75 star rated reviews than for reviews with 4.0 stars.</p> <p></p>"},{"location":"glassdoor_reviews/readme/#vader-and-sentiment-analysis","title":"VADER and Sentiment Analysis","text":"<p>VADER Sentiment Analysis. VADER (Valence Aware Dictionary and Sentiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media, and works well on texts from other domains.</p> <p>It\u2019s important to understand the output VADER provides: it is a Python dictionary with keys \u2018neg\u2019, \u2018neu\u2019, \u2018pos\u2019 \u2014 which correspond to Negative, Neutral, and Positive. It also has a Compound score which is a score calculated by normalizing the other 3 scores (neg, neu, pos). This score tells us the intensity or degree of the sentiment and not the actual values. Its values range from -1 (for extreme negative sentiment) to +1 (for extreme positive sentiment).</p> <p>Using the compound score can be enough to determine the underlying sentiment of a text, because for:</p> <p>a positive sentiment, compound \u2265 0.05 a negative sentiment, compound \u2264 -0.05 a neutral sentiment, the compound is between [-0.05, 0.05]</p> <pre><code># initialize sentiment analyzer\nsid_obj = SentimentIntensityAnalyzer()\n# going back to our sample string\n# it has a high compound score! it's positive\nsid_obj.polarity_scores(sample_string)\n</code></pre> <p>Here is our sample string:</p> <p>'good place retire since benefits good willing sacrifice salary/growth reviews great culture open learning environment reviews nice people work reviews very flexible good work life balance reviews good pay good hours reviews long hours culture staying late reviews difficult maintain good work/life balance reviews bad pay promotion hard get reviews'</p> <p>And the sentiment analyzer output: {'neg': 0.137, 'neu': 0.469, 'pos': 0.394, 'compound': 0.9646}</p> <p>Once we get all the compound, positive and negative scores we can plot them each against the stars given:</p> <p>Since compound scores greater than or equal to 0.5 are considered positive we should see a more linear relationship:</p> <p></p> <p>Here we see the positive scores vs. stars given: </p> <p></p> <p>And lastly, here are the negative scores vs. stars given: </p>"}]}